{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch import optim\n",
    "# from torchsummary import summary\n",
    "import gc\n",
    "from fastai.callback import *\n",
    "from fastai.utils.mem import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callbacks.hooks import hook_outputs\n",
    "import numpy as np\n",
    "import loader\n",
    "import fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-trained VGG16 (with  batch norm) for feature loss\n",
    "from torchvision.models import vgg16_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the ranger optimizer\n",
    "# https://medium.com/@lessw/new-deep-learning-optimizer-ranger-synergistic-combination-of-radam-lookahead-for-the-best-of-2dc83f79a48d\n",
    "# code: https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n",
    "from ranger import Ranger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=8 #batch size\n",
    "size=128 # image size (size x size)\n",
    "\n",
    "# image paths\n",
    "path_out = 'data/JackieChan2A'\n",
    "path_inp = 'data/A2JackieChan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gpu if possible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Original dataloading (doesn't work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# create image 2 image databunch\\n# code from: ???\\nsrc = ImageImageList.from_folder(path_inp).split_by_rand_pct(0.05, seed=42)\\ndef get_data(bs,size):\\n    data = (src.label_from_func(lambda x: path_out+'/'+x.name)\\n           .transform(get_transforms(), size=size, tfm_y=True)\\n           .databunch(bs=bs).normalize(imagenet_stats, do_y=True))\\n\\n    data.c = 3\\n    return data\\ndata = get_data(bs,size)\\ndata.show_batch(ds_type=DatasetType.Valid, rows=2, figsize=(9,9))\\ndata\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# create image 2 image databunch\n",
    "# code from: ???\n",
    "src = ImageImageList.from_folder(path_inp).split_by_rand_pct(0.05, seed=42)\n",
    "def get_data(bs,size):\n",
    "    data = (src.label_from_func(lambda x: path_out+'/'+x.name)\n",
    "           .transform(get_transforms(), size=size, tfm_y=True)\n",
    "           .databunch(bs=bs).normalize(imagenet_stats, do_y=True))\n",
    "\n",
    "    data.c = 3\n",
    "    return data\n",
    "data = get_data(bs,size)\n",
    "data.show_batch(ds_type=DatasetType.Valid, rows=2, figsize=(9,9))\n",
    "data'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Looks like some errors made it into the data set. The first target image isn't Jackie Chan, but his son (I think)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data/dataloaders (MD/PS work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Img_Dataset(Dataset):\n",
    "    def __init__(self, data_set, patch_size, width, height, seed=1234):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data: np.ndarray\n",
    "            Array that contains image/label pairs ie. corrupted image/clean image.\n",
    "            Shape = (P, N, C, H, W):\n",
    "                P = corrupted/uncorrupted image pair \n",
    "                N = number of samples\n",
    "                C = number of channels\n",
    "                H = image height\n",
    "                W = image width\n",
    "        patch_size: int\n",
    "            Size of randomly chosen image patch the model uses for training\n",
    "        width: int\n",
    "            Width of the chosen sample.\n",
    "            NOTE: It's a parameter because you can input a larger image and choose\n",
    "                  to look at only portions of said image for more training samples.\n",
    "        height: int\n",
    "            Height of the chosen sample.\n",
    "        seed: int \n",
    "            Randomized seed used for the random slicing used to create the image patch.\n",
    "        \"\"\"\n",
    "        self.data_set = data_set\n",
    "        self.patch_size = patch_size\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.seed = seed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_set[0])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Function that returns the PyTorch Dataloader compatible dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        idx: var\n",
    "            Variable used in PyTorch Dataloader to be able to sample from the dataset\n",
    "            to create minibatches of the data for us automatically.\n",
    "        \"\"\"\n",
    "        # Loading the dataset and then slicing the image/label pairs \n",
    "        # ie. corrupted/uncorrupted images. \n",
    "        # Note the use of the idx in the image/label variables. This allows the\n",
    "        # PyTorch Dataloader to get all the important data info eg. (N, C, H, W)\n",
    "        data = self.data_set\n",
    "        image = data[0, idx]\n",
    "        label = data[1, idx]\n",
    "        \n",
    "        # Setting the patch size and the randomized seed for the image patch\n",
    "        patch_size = self.patch_size\n",
    "        seed = self.seed\n",
    "        rng = np.random.RandomState(seed)\n",
    "\n",
    "        img_width = self.width\n",
    "        img_height = self.height\n",
    "        \n",
    "        #randomly crop patch from training set\n",
    "        x1 = rng.randint(img_width - patch_size)\n",
    "        y1 = rng.randint(img_height - patch_size)\n",
    "        S = (slice(y1, y1 + patch_size), slice(x1, x1 + patch_size))\n",
    "        \n",
    "        # create new arrays for training patchs\n",
    "        image_patch = image[0][S]\n",
    "        label_patch = label[0][S]\n",
    "        \n",
    "\n",
    "        image_patch = image_patch[np.newaxis, :, :]\n",
    "        label_patch = label_patch[np.newaxis, :, :]\n",
    "        \n",
    "        image_patch = np.concatenate((image_patch,)*3, axis=0)\n",
    "        label_patch = np.concatenate((label_patch,)*3, axis=0)\n",
    "\n",
    "        \n",
    "        # Turning our image/label to a PyTorch Tensor with dtype = float \n",
    "        # and then putting it onto the GPU for faster training/inference\n",
    "        \n",
    "        image = torch.from_numpy(image_patch).float().to(device)\n",
    "        label = torch.from_numpy(label_patch).float().to(device)\n",
    "        # image = torch.from_numpy(image_patch).float()\n",
    "        # label = torch.from_numpy(label_patch).float()\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train set= (2, 610, 1, 2000, 2000)\n"
     ]
    }
   ],
   "source": [
    "#Load the actual data that we're working on & print the shape of this data\n",
    "train_data = loader.load('training_data610-2000.npy')\n",
    "test_data = loader.load('test_data200-2000.npy')\n",
    "print('Shape of train set=', train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Img_Dataset(data_set=train_data,\n",
    "                       patch_size=64,\n",
    "                       height=2000,\n",
    "                       width=2000)\n",
    "\n",
    "test_ds = Img_Dataset(data_set=test_data,\n",
    "                       patch_size=64,\n",
    "                       height=2000,\n",
    "                       width=2000)\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, batch_size=56, shuffle=True)\n",
    "test_dataloader = DataLoader(test_ds, batch_size=56, shuffle=True)\n",
    "\n",
    "data = fastai.basic_data.DataBunch(train_dataloader, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to original work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictive filter flow layer\n",
    "# https://arxiv.org/abs/1811.11482\n",
    "# Kong, S., & Fowlkes, C. (2018). Image reconstruction with predictive filter flow. arXiv preprint arXiv:1811.11482.\n",
    "# - learn and apply individual filters (ksize x ksize) for each spatial position in the input\n",
    "# - i.e. when using softmax activation it is basically image warping, but instead of offsets, we learn filters\n",
    "class pFF(nn.Module):\n",
    "    def __init__(self,ni, ksize=3,stride=1,softmax = True,upsample=1):\n",
    "        super(pFF, self).__init__()\n",
    "        # size of the learned filter: ksize x ksize\n",
    "        self.ksize=ksize\n",
    "        # use softmax or tanh\n",
    "        self.softmax = softmax\n",
    "        # upsampling of the learned filters (gives smoother result)\n",
    "        self.upsample = upsample\n",
    "        # train conv layer to output filter flow and use reflection padding\n",
    "        self.get_filter = nn.Conv2d(ni,ksize**2,3,padding=1,stride=upsample,padding_mode='reflect')\n",
    "        self.pad = nn.ReflectionPad2d(padding=int((ksize-1)/2)*stride)\n",
    "        # apply learned filters\n",
    "        self.uf1 = nn.Unfold(ksize, dilation=stride, padding=0, stride=1)\n",
    "        self.uf2 = nn.Unfold(1, dilation=1, padding=0, stride=1)\n",
    "        if upsample>1:\n",
    "            self.us = nn.UpsamplingBilinear2d(scale_factor=upsample)\n",
    "        \n",
    "    def forward(self, features,inpt):\n",
    "        # features: features learned by CNN, inpt: input that filters should be applied to\n",
    "        # 1: get filter\n",
    "        ff = self.get_filter(features)\n",
    "        # 2: apply activation function\n",
    "        if self.softmax:\n",
    "            ff = F.softmax(ff,dim=1)\n",
    "        else:\n",
    "            ff = torch.tanh(ff)\n",
    "        if self.upsample>1:\n",
    "            ff = self.us(ff)\n",
    "            \n",
    "        # apply learned filters\n",
    "        inp_pad = self.pad(inpt)\n",
    "        \n",
    "        # use filter on each channel/feature of the input\n",
    "        ff = torch.cat([ff]*inpt.shape[1],dim=1)\n",
    "        out = self.uf1(inp_pad) * self.uf2(ff)\n",
    "        out = out.view(-1,inpt.shape[1],self.ksize**2,inpt.shape[2],inpt.shape[3])\n",
    "        return out.sum(dim=2)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build U-Net for face2face translation usining predictive filter flow\n",
    "# U-Net adapted from https://github.com/milesial/Pytorch-UNet\n",
    "# - use residual blocks instead of double convolution\n",
    "# - replaced transpose conv with PixelShuffle for efficiency\n",
    "# - replaced conv2d with depthwise separable conv for efficiency\n",
    "# - Use Mish activation function: code by https://github.com/lessw2020/mish\n",
    "# - multiple residual blocks in the middle\n",
    "# - predictive filter flow\n",
    "\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    # source: https://github.com/lessw2020/mish\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        #inlining this saves 1 second per epoch (V100 GPU) vs having a temp x and then returning x(!)\n",
    "        return x *( torch.tanh(F.softplus(x)))\n",
    "    \n",
    "\n",
    "class depthwise_separable_conv(nn.Module):\n",
    "    # source: \"shicai\": https://discuss.pytorch.org/t/how-to-modify-a-conv2d-to-depthwise-separable-convolution/15843)\n",
    "    def __init__(self, nin, nout,stride=1,ksize=3):\n",
    "        super(depthwise_separable_conv, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(nin, nin, kernel_size=ksize, \n",
    "                                   padding=int((ksize-1)/2), groups=nin,\n",
    "                                   stride=stride,padding_mode='reflect')\n",
    "        self.pointwise = nn.Conv2d(nin, nout, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out\n",
    "\n",
    "class block1(nn.Module):\n",
    "    def __init__(self,ni,no,stride=2,last_act=True):\n",
    "        super(block1, self).__init__()\n",
    "        self.bottleneck = ni != no\n",
    "        self.last_act = last_act\n",
    "        self.stride=stride\n",
    "        self.sconv1 = depthwise_separable_conv(ni,no,stride)\n",
    "        self.sconv2 = depthwise_separable_conv(no,no)\n",
    "        self.sconv3 = depthwise_separable_conv(no,no)\n",
    "        self.normact0 = nn.Sequential(nn.BatchNorm2d(no),Mish())\n",
    "        self.normact1 = nn.Sequential(nn.BatchNorm2d(no),Mish())\n",
    "        self.normact2 = nn.Sequential(nn.BatchNorm2d(no),Mish())\n",
    "    def forward(self,x):\n",
    "        if self.stride>1 or self.bottleneck:\n",
    "            x = self.normact0(self.sconv1(x))\n",
    "        residual = x\n",
    "        out = self.normact1(self.sconv2(x))\n",
    "        out = self.sconv3(out)\n",
    "        out += residual\n",
    "        if self.last_act:\n",
    "            out = self.normact2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class inconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(inconv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            depthwise_separable_conv(in_ch, out_ch,1,7),\n",
    "            Mish()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(down, self).__init__()\n",
    "        self.conv = block1(in_ch,out_ch,2)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class res_block(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super(res_block, self).__init__()\n",
    "        self.resconv = nn.Sequential(\n",
    "            block1(in_ch,in_ch,1,False),\n",
    "            block1(in_ch,in_ch,1,False),\n",
    "            block1(in_ch,in_ch,1,False),\n",
    "            block1(in_ch,in_ch,1,False)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.resconv(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch,mid_ch, out_ch, bilinear=True):\n",
    "        super(up, self).__init__()\n",
    "        # theoretically it makes sense to add another conv layer before pixel\n",
    "        # shuffle since we need a positional encoding, but here I did not find\n",
    "        # it to be necessary\n",
    "#         self.conv1 = nn.Conv2d(in_ch, in_ch, kernel_size=1)\n",
    "        self.ups = nn.modules.PixelShuffle(2)\n",
    "        self.norm = nn.BatchNorm2d(in_ch//4+mid_ch)\n",
    "        self.conv2 = block1(in_ch//4+mid_ch, out_ch,1)\n",
    "    def forward(self, x1, x2):\n",
    "        # double spatial resolution via pixel shuffle\n",
    "#         x1 = self.conv1(x1) \n",
    "        x1 = self.ups(x1)\n",
    "        # combine information of high- and low-level features\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.norm(x)\n",
    "        x = self.conv2(x)\n",
    "        return x  \n",
    "    \n",
    "    \n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        self.inc = inconv(3, 64)\n",
    "        self.down1 = down(64, 64)\n",
    "        self.down2 = down(64, 128)\n",
    "        self.down3 = down(128,256)\n",
    "        self.down4 = down(256,256)\n",
    "        self.res1 = res_block(256)\n",
    "        \n",
    "        self.up1 = up(256,256, 256)\n",
    "        self.up2 = up(256,128, 128)\n",
    "        self.up3 = up(128,64,64)\n",
    "        self.up4 = up(64,64, 64)\n",
    "        \n",
    "        self.filter0 = pFF(64, ksize=9)\n",
    "        self.filter1 = pFF(64, ksize=9,stride=4)\n",
    "        self.filter2 = pFF(64, ksize=9,stride=8)\n",
    "        self.filter3 = pFF(64, ksize=9)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        x1 = self.inc(inp)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.res1(x5)\n",
    "        \n",
    "        x = self.up1(x, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        filtered = self.filter0(x,inp)\n",
    "        filtered = self.filter1(x,filtered)\n",
    "        filtered = self.filter2(x,filtered)\n",
    "        filtered = self.filter3(x,filtered)\n",
    "    \n",
    "        return filtered#torch.sigmoid(x)*4-1.6 #x\n",
    "\n",
    "model = UNet()\n",
    "model = model.to(device)\n",
    "# summary(model, input_size=(3, size,size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBunch;\n",
       "\n",
       "Train: <__main__.Img_Dataset object at 0x7fd7d46cb2b0>;\n",
       "\n",
       "Valid: <__main__.Img_Dataset object at 0x7fd7d46cb310>;\n",
       "\n",
       "Test: None"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0735,  0.0923,  0.1194,  0.0260, -0.0513,  0.1205, -0.0798],\n",
       "          [ 0.0316,  0.1201,  0.1090,  0.0218,  0.1378, -0.0743, -0.0557],\n",
       "          [ 0.0671, -0.0775,  0.0115,  0.1103, -0.0321,  0.0138,  0.1135],\n",
       "          [ 0.0686,  0.1268,  0.0813, -0.1070, -0.1342, -0.0828,  0.0453],\n",
       "          [-0.1066, -0.0013,  0.1090,  0.1390, -0.0726,  0.0512,  0.0838],\n",
       "          [-0.0030, -0.0791, -0.0056,  0.0673,  0.1043,  0.0451, -0.0156],\n",
       "          [ 0.0361, -0.0817,  0.1336,  0.1220, -0.0308,  0.1421, -0.0596]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1381,  0.0836, -0.0523,  0.1165, -0.1349,  0.0113, -0.0289],\n",
       "          [-0.0737, -0.0852,  0.0037,  0.0063, -0.0229,  0.0264, -0.0277],\n",
       "          [ 0.0249, -0.1395,  0.0210, -0.1040, -0.1241, -0.1128,  0.0267],\n",
       "          [-0.0699, -0.1338, -0.1163, -0.0858, -0.0710, -0.0193, -0.0326],\n",
       "          [-0.0585, -0.0615,  0.0898, -0.0370, -0.0671, -0.0285, -0.1272],\n",
       "          [ 0.1026, -0.0805,  0.0980, -0.0636,  0.1009,  0.1081,  0.1229],\n",
       "          [-0.0817, -0.1051, -0.0230,  0.0125,  0.1022, -0.0900,  0.0455]]],\n",
       "\n",
       "\n",
       "        [[[-0.0737, -0.0126, -0.0524, -0.0587,  0.1073,  0.1118, -0.1372],\n",
       "          [-0.0688,  0.1115, -0.0748, -0.0443,  0.1365,  0.0612,  0.0155],\n",
       "          [ 0.1013,  0.1366,  0.0694, -0.1049,  0.1337, -0.0432,  0.0586],\n",
       "          [ 0.0611, -0.1300,  0.0234,  0.1003, -0.0629, -0.0371, -0.0965],\n",
       "          [ 0.1241,  0.1003,  0.1370,  0.0990,  0.0831, -0.0871,  0.0458],\n",
       "          [-0.0695,  0.0441,  0.1075, -0.0267, -0.0813, -0.1229,  0.0911],\n",
       "          [-0.0405,  0.1250, -0.1184, -0.1351,  0.0971, -0.0187,  0.0043]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = model.state_dict()\n",
    "x['inc.conv.0.depthwise.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def charbonnier(y_pred, y_true):\n",
    "    epsilon = 1e-3\n",
    "    error = y_true - y_pred\n",
    "    p = torch.sqrt(error**2 + epsilon**2)\n",
    "    return torch.mean(p)\n",
    "\n",
    "\n",
    "# Perceptual Loss:\n",
    "# original code from: fast.ai lesson ???\n",
    "# modifications:\n",
    "# - Instance Normalization of low-level features to remove influence of \"style\"\n",
    "def gram_matrix(x):\n",
    "    n,c,h,w = x.size()\n",
    "    x = x.view(n, c, -1)\n",
    "    return (x @ x.transpose(1,2))/(c*h*w)\n",
    "#vgg_m = vgg16_bn(True).features.cuda().eval()\n",
    "vgg_m = vgg16_bn(True).features.to(device).eval()\n",
    "requires_grad(vgg_m, False)\n",
    "blocks = [i-1 for i,o in enumerate(children(vgg_m)) if isinstance(o,nn.MaxPool2d)]\n",
    "\n",
    "base_loss = F.mse_loss\n",
    "\n",
    "class FeatureLoss(nn.Module):\n",
    "    def __init__(self, m_feat, layer_ids, layer_wgts,without_instancenorm=1):\n",
    "        super().__init__()\n",
    "        self.m_feat = m_feat\n",
    "        # how many layers are not subjected to instance norm (starting from high-level, i.e. later layers)\n",
    "        self.without_instancenorm = without_instancenorm\n",
    "        self.loss_features = [self.m_feat[i] for i in layer_ids]\n",
    "        self.hooks = hook_outputs(self.loss_features, detach=False)\n",
    "        self.wgts = layer_wgts # This is a list. How to get this onto GPU\n",
    "        self.metric_names = [f'feat_{i}' for i in range(len(layer_ids))\n",
    "                                         ]+ [f'gram_{i}' for i in range(len(layer_ids))]\n",
    "              \n",
    "\n",
    "    def make_features(self, x, clone=False):\n",
    "        self.m_feat(x)\n",
    "        return [(o.clone() if clone else o) for o in self.hooks.stored]\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        out_feat = self.make_features(target, clone=True)\n",
    "        in_feat = self.make_features(input)\n",
    "        try:\n",
    "            # instance normalization for all but last layer\n",
    "            for l in range(len(in_feat)-self.without_instancenorm):\n",
    "                in_feat[l] = nn.InstanceNorm2d(in_feat[l][1],momentum=0)(in_feat[l])\n",
    "                out_feat[l] = nn.InstanceNorm2d(out_feat[l][1],momentum=0)(out_feat[l])\n",
    "                \n",
    "            self.feat_losses = [base_loss(f_in, f_out)*w\n",
    "                                 for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]\n",
    "            self.feat_losses += [base_loss(gram_matrix(f_in), gram_matrix(f_out))*w**2 * 5e3\n",
    "                                 for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]\n",
    "        except IndexError: # No idea why this tends to happen (only during validation)\n",
    "            self.feat_losses = [torch.tensor(1).float().to(device)]\n",
    "            for k in range(6):\n",
    "                self.feat_losses += [torch.tensor(1).float().to(device)]\n",
    "        self.metrics = dict(zip(self.metric_names, self.feat_losses))\n",
    "        return sum(self.feat_losses)\n",
    "    \n",
    "    def __del__(self): self.hooks.remove()\n",
    "        \n",
    "        \n",
    "feat_loss = FeatureLoss(vgg_m, blocks[2:5], [2,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = Learner(data,model,loss_func=feat_loss, opt_func=Ranger)\n",
    "gc.collect()\n",
    "# show output before training\n",
    "# G.show_results(rows=2, imgsize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/10 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='0' class='progress-bar-interrupted' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      Interrupted\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set state called\n",
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22584/1534921265.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# find good learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/perlmutter/pytorch1.9.0/lib/python3.8/site-packages/fastai/train.py\u001b[0m in \u001b[0;36mlr_find\u001b[0;34m(learn, start_lr, end_lr, num_it, stop_div, wd)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLRFinder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_it\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_div\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_it\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_distrib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m def to_fp16(learn:Learner, loss_scale:float=None, max_noskip:int=1000, dynamic:bool=True, clip:float=None,\n",
      "\u001b[0;32m~/.local/perlmutter/pytorch1.9.0/lib/python3.8/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/perlmutter/pytorch1.9.0/lib/python3.8/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/perlmutter/pytorch1.9.0/lib/python3.8/site-packages/fastprogress/fastprogress.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_interrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/perlmutter/pytorch1.9.0/lib/python3.8/site-packages/fastprogress/fastprogress.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/perlmutter/pytorch1.9.0/lib/python3.8/site-packages/fastai/basic_data.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;34m\"Process and returns items from `DataLoader`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/software/nersc/shasta2105/pytorch/1.9.0/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/software/nersc/shasta2105/pytorch/1.9.0/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/software/nersc/shasta2105/pytorch/1.9.0/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py\u001b[0m in \u001b[0;36mpin_memory\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pin_memory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/software/nersc/shasta2105/pytorch/1.9.0/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pin_memory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/software/nersc/shasta2105/pytorch/1.9.0/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py\u001b[0m in \u001b[0;36mpin_memory\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned"
     ]
    }
   ],
   "source": [
    "# find good learning rate\n",
    "G.lr_find()\n",
    "G.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "G.fit_fc(15,4e-2)\n",
    "# show intermediate results\n",
    "G.show_results(rows=2, imgsize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "G.fit_fc(15,4e-3)\n",
    "# show intermediate result\n",
    "G.show_results(rows=2, imgsize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "G.fit_fc(15,4e-4)\n",
    "# show final result\n",
    "G.show_results(rows=2, imgsize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "G.save(path_inp.split('/')[-1])\n",
    "G.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Show import live_swap\n",
    "# model2 = load_learner(path_inp)\n",
    "# live_swap(model2,size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-1.9.0",
   "language": "python",
   "name": "pytorch-1.9.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
