{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e2af089-3fd0-476e-905a-da5ab8f4e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib \n",
    "import os\n",
    "from scipy import signal\n",
    "\n",
    "import torch\n",
    "\n",
    "import PT_files.save_load as sl\n",
    "from DnCNN_NP.layers  import relu#, np_BatchNorm2d, np_Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c18ad9c-cde3-4e79-9555-7d39a1866b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_numpy_comparison(input_data,\n",
    "                             pytorch_output,\n",
    "                             numpy_output,\n",
    "                             sample_idx):\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(1, 3, figsize=(24,20))\n",
    "    vmin, vmax = np.percentile(input_data[sample_idx], (1,99))\n",
    "\n",
    "    ax[0].imshow(pytorch_output[sample_idx][0], vmin=vmin, vmax=vmax, origin='lower', interpolation='none')\n",
    "    ax[0].axis('off')\n",
    "    ax[0].set_title('Pytorch BatchNorm', fontsize=30)\n",
    "    ax[1].imshow(input_data[sample_idx][0],vmin=vmin, vmax=vmax, origin='lower', interpolation='none')\n",
    "    ax[1].axis('off')\n",
    "    ax[1].set_title('Input Sample', fontsize=30)\n",
    "    ax[2].imshow(numpy_output[sample_idx][0], vmin=vmin, vmax=vmax, origin='lower', interpolation='none')\n",
    "    ax[2].axis('off')\n",
    "    ax[2].set_title('Numpy BatchNorm', fontsize=30)\n",
    "    \n",
    "    \n",
    "def full_grid_pass_window(dataset, model, model_params, samp_idx):\n",
    "    \n",
    "    full = np.empty((1, 1, 6000,6000))\n",
    "    count = np.empty((1, 1, 6000,6000))\n",
    "    \n",
    "    noise_data = dataset[0]\n",
    "    param_name = model_params\n",
    "    print('pass 1')\n",
    "    \n",
    "    current_dir = pathlib.Path().resolve()\n",
    "    model_params_path = current_dir / 'Model_params'\n",
    "    assert model_params_path.exists()\n",
    "    model_path = model_params_path / param_name\n",
    "    print('pass 2')\n",
    "    \n",
    "    model = model()\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load(str(model_path)))\n",
    "    model.eval();\n",
    "    print('pass 3')\n",
    "    # telling pytorch this is for inference and not learning, so keeps the weights unchanged\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        print('pass 4')\n",
    "        torch.cuda.empty_cache()\n",
    "        test_noise = torch.as_tensor(noise_data[samp_idx:samp_idx+1,:,:, :])\n",
    "        test_noise = test_noise.to(device)\n",
    "\n",
    "        print('pass 5')\n",
    "        output = model(test_noise)\n",
    "        print('pass 6')\n",
    "        resid_img = output.detach().cpu().numpy()\n",
    "        print('pass 7')\n",
    "\n",
    "        full[:, :, :, :] += resid_img\n",
    "\n",
    "    return full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43273a8f-4130-4f26-a07c-9c4db85c5cd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test set= (108, 1, 6000, 6000)\n"
     ]
    }
   ],
   "source": [
    "PATH = pathlib.Path(os.getenv('PSCRATCH'))\n",
    "DATA = PATH / 'DESI_dn' /'Model_params'\n",
    "assert DATA.exists()\n",
    "name = '6k_model_wb_e800_lys20_58feat.pth'\n",
    "\n",
    "# weights = np.load(DATA / name)\n",
    "weights = torch.load(str(DATA / name))\n",
    "\n",
    "\n",
    "#Load the actual data that we're working on & print the shape of this data\n",
    "test_data = sl.NERSC_load('test_data_40%_6000.npy')\n",
    "sample = test_data[0]\n",
    "print('Shape of test set=', sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96fccdee-1e8c-48cb-8fb5-8621fc9eb25a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['layers.0.0.weight', 'layers.0.0.bias', 'layers.1.0.weight', 'layers.1.0.bias', 'layers.1.1.weight', 'layers.1.1.bias', 'layers.1.1.running_mean', 'layers.1.1.running_var', 'layers.1.1.num_batches_tracked', 'layers.2.0.weight', 'layers.2.0.bias', 'layers.2.1.weight', 'layers.2.1.bias', 'layers.2.1.running_mean', 'layers.2.1.running_var', 'layers.2.1.num_batches_tracked', 'layers.3.0.weight', 'layers.3.0.bias', 'layers.3.1.weight', 'layers.3.1.bias', 'layers.3.1.running_mean', 'layers.3.1.running_var', 'layers.3.1.num_batches_tracked', 'layers.4.0.weight', 'layers.4.0.bias', 'layers.4.1.weight', 'layers.4.1.bias', 'layers.4.1.running_mean', 'layers.4.1.running_var', 'layers.4.1.num_batches_tracked', 'layers.5.0.weight', 'layers.5.0.bias', 'layers.5.1.weight', 'layers.5.1.bias', 'layers.5.1.running_mean', 'layers.5.1.running_var', 'layers.5.1.num_batches_tracked', 'layers.6.0.weight', 'layers.6.0.bias', 'layers.6.1.weight', 'layers.6.1.bias', 'layers.6.1.running_mean', 'layers.6.1.running_var', 'layers.6.1.num_batches_tracked', 'layers.7.0.weight', 'layers.7.0.bias', 'layers.7.1.weight', 'layers.7.1.bias', 'layers.7.1.running_mean', 'layers.7.1.running_var', 'layers.7.1.num_batches_tracked', 'layers.8.0.weight', 'layers.8.0.bias', 'layers.8.1.weight', 'layers.8.1.bias', 'layers.8.1.running_mean', 'layers.8.1.running_var', 'layers.8.1.num_batches_tracked', 'layers.9.0.weight', 'layers.9.0.bias', 'layers.9.1.weight', 'layers.9.1.bias', 'layers.9.1.running_mean', 'layers.9.1.running_var', 'layers.9.1.num_batches_tracked', 'layers.10.0.weight', 'layers.10.0.bias', 'layers.10.1.weight', 'layers.10.1.bias', 'layers.10.1.running_mean', 'layers.10.1.running_var', 'layers.10.1.num_batches_tracked', 'layers.11.0.weight', 'layers.11.0.bias', 'layers.11.1.weight', 'layers.11.1.bias', 'layers.11.1.running_mean', 'layers.11.1.running_var', 'layers.11.1.num_batches_tracked', 'layers.12.0.weight', 'layers.12.0.bias', 'layers.12.1.weight', 'layers.12.1.bias', 'layers.12.1.running_mean', 'layers.12.1.running_var', 'layers.12.1.num_batches_tracked', 'layers.13.0.weight', 'layers.13.0.bias', 'layers.13.1.weight', 'layers.13.1.bias', 'layers.13.1.running_mean', 'layers.13.1.running_var', 'layers.13.1.num_batches_tracked', 'layers.14.0.weight', 'layers.14.0.bias', 'layers.14.1.weight', 'layers.14.1.bias', 'layers.14.1.running_mean', 'layers.14.1.running_var', 'layers.14.1.num_batches_tracked', 'layers.15.0.weight', 'layers.15.0.bias', 'layers.15.1.weight', 'layers.15.1.bias', 'layers.15.1.running_mean', 'layers.15.1.running_var', 'layers.15.1.num_batches_tracked', 'layers.16.0.weight', 'layers.16.0.bias', 'layers.16.1.weight', 'layers.16.1.bias', 'layers.16.1.running_mean', 'layers.16.1.running_var', 'layers.16.1.num_batches_tracked', 'layers.17.0.weight', 'layers.17.0.bias', 'layers.17.1.weight', 'layers.17.1.bias', 'layers.17.1.running_mean', 'layers.17.1.running_var', 'layers.17.1.num_batches_tracked', 'layers.18.0.weight', 'layers.18.0.bias', 'layers.18.1.weight', 'layers.18.1.bias', 'layers.18.1.running_mean', 'layers.18.1.running_var', 'layers.18.1.num_batches_tracked', 'layers.19.weight', 'layers.19.bias'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe156740-2c42-4a02-b5aa-1be0d1904153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_Conv2d(input_data, weights_dict, prefix, stride=1, padding=\"same\", dilation=1):\n",
    "    \n",
    "    batch_size, input_channels, height, width = input_data.shape # (N, Cin, Hin, Win)\n",
    "    \n",
    "    weight = weights_dict[str(prefix) + 'weight']\n",
    "    bias = weights_dict[str(prefix) + 'bias']\n",
    "    \n",
    "    kernel_size = weight[0][0].shape\n",
    "    output_channels = len(weight)\n",
    "    \n",
    "    # Convert string padding into numerical padding\n",
    "    # Using strings allow for one variable to account for padding & mode (see signal.correlated2d)\n",
    "    mode = padding\n",
    "    if mode == \"same\":\n",
    "        padding = 1\n",
    "    elif mode == \"valid\":\n",
    "        padding = 0\n",
    "    \n",
    "    height_out = ((height + (2*padding) - dilation * (kernel_size[0] - 1) - 1) / stride) + 1\n",
    "    height_out = int(height_out)\n",
    "    width_out = ((width + (2*padding) - dilation * (kernel_size[1] - 1) - 1) / stride) + 1\n",
    "    width_out = int(width_out)\n",
    "\n",
    "    output = np.empty((batch_size, output_channels, height_out, width_out))\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        for j in range(output_channels):\n",
    "            output[i, j, :, :] = bias[j].detach().cpu().numpy() + signal.correlate2d(input_data[i][0], weight[j][0].detach().cpu().numpy(), mode=mode)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def np_BatchNorm2d(input_data, prefix, weights_dict, epsilon=1e-5):\n",
    "    \n",
    "    x = input_data\n",
    "    \n",
    "    gamma = weights_dict[str(prefix) + 'weight'].detach().cpu().numpy().reshape(-1, 1, 1)\n",
    "    beta = weights_dict[str(prefix) + 'bias'].detach().cpu().numpy().reshape(-1, 1, 1)\n",
    "    mean = weights_dict[str(prefix) + 'running_mean'].detach().cpu().numpy().reshape(-1, 1, 1)\n",
    "    var = weights_dict[str(prefix) + 'running_var'].detach().cpu().numpy().reshape(-1, 1, 1)\n",
    "        \n",
    "        \n",
    "    output = ((x - mean) / np.sqrt(var + epsilon)) * gamma + beta\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5e81bb5-9dd8-4517-bbf9-70f055abd19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(data, prefix, weights_dict):\n",
    "    \n",
    "    if type(prefix) != str:\n",
    "        raise Exception(\"The prefix must be a string!\")\n",
    "                        \n",
    "    if prefix.endswith('0.'):\n",
    "        conv_layer_output = np_Conv2d(input_data=data,\n",
    "                                      weights_dict=weights_dict,\n",
    "                                      prefix=prefix)\n",
    "        return conv_layer_output\n",
    "    \n",
    "    elif prefix.endswith('1.'):\n",
    "        batch_layer_output = np_BatchNorm2d(x=data,\n",
    "                                            prefix=prefix,\n",
    "                                            weights_dict=weights_dict)\n",
    "        return batch_layer_output\n",
    "    \n",
    "    elif prefix.endswith('19.'):\n",
    "        conv_layer_output = np_Conv2d(input_data=data,\n",
    "                                      weights_dict=weights_dict,\n",
    "                                      prefix=prefix)\n",
    "        return conv_layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4093f786-d62b-4ac7-8d0d-869b3b2268c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['layers.0.0.', 'layers.1.0.', 'layers.1.1.', 'layers.2.0.', 'layers.2.1.', 'layers.3.0.', 'layers.3.1.', 'layers.4.0.', 'layers.4.1.', 'layers.5.0.', 'layers.5.1.', 'layers.6.0.', 'layers.6.1.', 'layers.7.0.', 'layers.7.1.', 'layers.8.0.', 'layers.8.1.', 'layers.9.0.', 'layers.9.1.', 'layers.10.0.', 'layers.10.1.', 'layers.11.0.', 'layers.11.1.', 'layers.12.0.', 'layers.12.1.', 'layers.13.0.', 'layers.13.1.', 'layers.14.0.', 'layers.14.1.', 'layers.15.0.', 'layers.15.1.', 'layers.16.0.', 'layers.16.1.', 'layers.17.0.', 'layers.17.1.', 'layers.18.0.', 'layers.18.1.', 'layers.19.']\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# Replace the last part of the key that describes what layer it is\n",
    "# part of and replaces it with empty space\n",
    "layers_list = [x.replace('weight', '').replace('bias', '').replace('running_mean', '').replace('running_var', '').replace('num_batches_tracked', '') for x in weights.keys()]\n",
    "# Convert this list which has duplicated elements due to removing\n",
    "# identifying elements ie. for the first conv layer we had\n",
    "# layers.0.0.weight & layers.0.0.bias, but now after removing them we\n",
    "# have layers.0.0 & layers.0.0\n",
    "# The code below deletes the duplicated elements\n",
    "layers_list = list(OrderedDict.fromkeys(layers_list))\n",
    "print(layers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e23978a-0cfa-4540-baab-aa67a39d6fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(layers_list)-2):\n",
    "#     print(layers_list[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a773063e-8725-48ea-b279-9b7e7c8f7fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv layer 61.02711102302419 seconds\n",
      "Conv layer 63.49673509300919 seconds\n",
      "Batch layer 25.360057851008605 seconds\n",
      "Conv layer 62.47572086501168 seconds\n",
      "Batch layer 25.20298542402452 seconds\n",
      "Conv layer 63.881072305026464 seconds\n",
      "Batch layer 25.345183948986232 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13913/871433757.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mconv_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m output = np_Conv2d(input_data=output,\n\u001b[0m\u001b[1;32m     60\u001b[0m                            \u001b[0mweights_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                            prefix=layers_list[7])\n",
      "\u001b[0;32m/tmp/ipykernel_13913/2137819419.py\u001b[0m in \u001b[0;36mnp_Conv2d\u001b[0;34m(input_data, weights_dict, prefix, stride, padding, dilation)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrelate2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/software/nersc/shasta2105/pytorch/1.9.0/lib/python3.8/site-packages/scipy/signal/signaltools.py\u001b[0m in \u001b[0;36mcorrelate2d\u001b[0;34m(in1, in2, mode, boundary, fillvalue)\u001b[0m\n\u001b[1;32m   1784\u001b[0m     \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_valfrommode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m     \u001b[0mbval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_bvalfromboundary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboundary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1786\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convolve2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfillvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mswapped_inputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "conv_start = time.perf_counter()\n",
    "output = np_Conv2d(input_data=sample[:1],\n",
    "                   weights_dict=weights,\n",
    "                   prefix=layers_list[0])\n",
    "conv_end = time.perf_counter()\n",
    "print('Conv layer', conv_end-conv_start, 'seconds')\n",
    "\n",
    "\n",
    "conv_start = time.perf_counter()\n",
    "output = np_Conv2d(input_data=output,\n",
    "                           weights_dict=weights,\n",
    "                           prefix=layers_list[1])\n",
    "conv_end = time.perf_counter()\n",
    "print('Conv layer', conv_end-conv_start, 'seconds')\n",
    "\n",
    "\n",
    "batch_start = time.perf_counter()\n",
    "output = np_BatchNorm2d(input_data=output,\n",
    "                            weights_dict=weights,\n",
    "                            prefix=layers_list[2])\n",
    "batch_end = time.perf_counter()\n",
    "output = relu(output)\n",
    "print('Batch layer', batch_end-batch_start, 'seconds')\n",
    "\n",
    "              \n",
    "conv_start = time.perf_counter()\n",
    "output = np_Conv2d(input_data=output,\n",
    "                           weights_dict=weights,\n",
    "                           prefix=layers_list[3])\n",
    "conv_end = time.perf_counter()\n",
    "print('Conv layer', conv_end-conv_start, 'seconds')\n",
    "\n",
    "batch_start = time.perf_counter()\n",
    "output = np_BatchNorm2d(input_data=output,\n",
    "                            weights_dict=weights,\n",
    "                            prefix=layers_list[4])\n",
    "batch_end = time.perf_counter()\n",
    "output = relu(output)\n",
    "print('Batch layer', batch_end-batch_start, 'seconds')\n",
    "              \n",
    "              \n",
    "              \n",
    "conv_start = time.perf_counter()\n",
    "output = np_Conv2d(input_data=output,\n",
    "                           weights_dict=weights,\n",
    "                           prefix=layers_list[5])\n",
    "conv_end = time.perf_counter()\n",
    "print('Conv layer', conv_end-conv_start, 'seconds')\n",
    "\n",
    "batch_start = time.perf_counter()\n",
    "output = np_BatchNorm2d(input_data=output,\n",
    "                            weights_dict=weights,\n",
    "                            prefix=layers_list[6])\n",
    "batch_end = time.perf_counter()\n",
    "output = relu(output)\n",
    "print('Batch layer', batch_end-batch_start, 'seconds')\n",
    "\n",
    "conv_start = time.perf_counter()\n",
    "output = np_Conv2d(input_data=output,\n",
    "                           weights_dict=weights,\n",
    "                           prefix=layers_list[7])\n",
    "conv_end = time.perf_counter()\n",
    "print('Conv layer', conv_end-conv_start, 'seconds')\n",
    "\n",
    "batch_start = time.perf_counter()\n",
    "output = np_BatchNorm2d(input_data=output,\n",
    "                            weights_dict=weights,\n",
    "                            prefix=layers_list[8])\n",
    "batch_end = time.perf_counter()\n",
    "output = relu(output)\n",
    "print('Batch layer', batch_end-batch_start, 'seconds')\n",
    "\n",
    "conv_start = time.perf_counter()\n",
    "output = np_Conv2d(input_data=output,\n",
    "                           weights_dict=weights,\n",
    "                           prefix=layers_list[9])\n",
    "conv_end = time.perf_counter()\n",
    "print('Conv layer', conv_end-conv_start, 'seconds')\n",
    "\n",
    "              \n",
    "batch_start = time.perf_counter()\n",
    "output = np_BatchNorm2d(input_data=output,\n",
    "                            weights_dict=weights,\n",
    "                            prefix=layers_list[10])\n",
    "batch_end = time.perf_counter()\n",
    "output = relu(output)\n",
    "print('Batch layer', batch_end-batch_start, 'seconds')\n",
    "\n",
    "              \n",
    "              \n",
    "conv_start = time.perf_counter()\n",
    "output = np_Conv2d(input_data=output,\n",
    "                           weights_dict=weights,\n",
    "                           prefix=layers_list[11])\n",
    "conv_end = time.perf_counter()\n",
    "print('Conv layer', conv_end-conv_start, 'seconds')\n",
    "\n",
    "batch_start = time.perf_counter()\n",
    "output = np_BatchNorm2d(input_data=output,\n",
    "                            weights_dict=weights,\n",
    "                            prefix=layers_list[12])\n",
    "batch_end = time.perf_counter()\n",
    "output = relu(output)\n",
    "print('Batch layer', batch_end-batch_start, 'seconds')\n",
    "\n",
    "\n",
    "              \n",
    "conv_start = time.perf_counter()\n",
    "output = np_Conv2d(input_data=output,\n",
    "                           weights_dict=weights,\n",
    "                           prefix=layers_list[13])\n",
    "conv_end = time.perf_counter()\n",
    "print('Conv layer', conv_end-conv_start, 'seconds')\n",
    "\n",
    "              \n",
    "batch_start = time.perf_counter()\n",
    "output = np_BatchNorm2d(input_data=output,\n",
    "                            weights_dict=weights,\n",
    "                            prefix=layers_list[14])\n",
    "batch_end = time.perf_counter()\n",
    "output = relu(output)\n",
    "print('Batch layer', batch_end-batch_start, 'seconds')\n",
    "\n",
    "conv_start = time.perf_counter()\n",
    "output = np_Conv2d(input_data=output,\n",
    "                           weights_dict=weights,\n",
    "                           prefix=layers_list[15])\n",
    "conv_end = time.perf_counter()\n",
    "print('Conv layer', conv_end-conv_start, 'seconds')\n",
    "\n",
    "              \n",
    "batch_start = time.perf_counter()\n",
    "output = np_BatchNorm2d(input_data=output,\n",
    "                            weights_dict=weights,\n",
    "                            prefix=layers_list[16])\n",
    "batch_end = time.perf_counter()\n",
    "output = relu(output)\n",
    "print('Batch layer', batch_end-batch_start, 'seconds')\n",
    "\n",
    "              \n",
    "              \n",
    "conv_start = time.perf_counter()\n",
    "output = np_Conv2d(input_data=output,\n",
    "                           weights_dict=weights,\n",
    "                           prefix=layers_list[17])\n",
    "conv_end = time.perf_counter()\n",
    "print('Conv layer', conv_end-conv_start, 'seconds')\n",
    "\n",
    "              \n",
    "              \n",
    "batch_start = time.perf_counter()\n",
    "output = np_BatchNorm2d(input_data=output,\n",
    "                            weights_dict=weights,\n",
    "                            prefix=layers_list[18])\n",
    "batch_end = time.perf_counter()\n",
    "output = relu(output)\n",
    "print('Batch layer', batch_end-batch_start, 'seconds')\n",
    "\n",
    "\n",
    "conv_start = time.perf_counter()\n",
    "output = np_Conv2d(input_data=output,\n",
    "                           weights_dict=weights,\n",
    "                           prefix=layers_list[19])\n",
    "conv_end = time.perf_counter()\n",
    "print('Conv layer', conv_end-conv_start, 'seconds')\n",
    "\n",
    "              \n",
    "              \n",
    "batch_start = time.perf_counter()\n",
    "output = np_BatchNorm2d(input_data=output,\n",
    "                            weights_dict=weights,\n",
    "                            prefix=layers_list[20])\n",
    "batch_end = time.perf_counter()\n",
    "output = relu(output)\n",
    "print('Batch layer', batch_end-batch_start, 'seconds')\n",
    "\n",
    "\n",
    "conv_start = time.perf_counter()\n",
    "output = np_Conv2d(input_data=output,\n",
    "                           weights_dict=weights,\n",
    "                           prefix=layers_list[21])\n",
    "conv_end = time.perf_counter()\n",
    "print('Conv layer', conv_end-conv_start, 'seconds')\n",
    "\n",
    "              \n",
    "              \n",
    "batch_start = time.perf_counter()\n",
    "output = np_BatchNorm2d(input_data=output,\n",
    "                            weights_dict=weights,\n",
    "                            prefix=layers_list[22])\n",
    "batch_end = time.perf_counter()\n",
    "output = relu(output)\n",
    "print('Batch layer', batch_end-batch_start, 'seconds')\n",
    "\n",
    "\n",
    "conv_start = time.perf_counter()\n",
    "output = np_Conv2d(input_data=output,\n",
    "                           weights_dict=weights,\n",
    "                           prefix=layers_list[23])\n",
    "conv_end = time.perf_counter()\n",
    "print('Conv layer', conv_end-conv_start, 'seconds')\n",
    "              \n",
    "              \n",
    "batch_start = time.perf_counter()\n",
    "output = np_BatchNorm2d(input_data=output,\n",
    "                            weights_dict=weights,\n",
    "                            prefix=layers_list[24])\n",
    "batch_end = time.perf_counter()\n",
    "output = relu(output)\n",
    "print('Batch layer', batch_end-batch_start, 'seconds')\n",
    "              \n",
    "\n",
    "conv_start = time.perf_counter()\n",
    "output = np_Conv2d(input_data=output,\n",
    "                           weights_dict=weights,\n",
    "                           prefix=layers_list[25])\n",
    "conv_end = time.perf_counter()\n",
    "print('Conv layer', conv_end-conv_start, 'seconds')\n",
    "              \n",
    "              \n",
    "batch_start = time.perf_counter()\n",
    "output = np_BatchNorm2d(input_data=output,\n",
    "                            weights_dict=weights,\n",
    "                            prefix=layers_list[26])\n",
    "batch_end = time.perf_counter()\n",
    "output = relu(output)\n",
    "print('Batch layer', batch_end-batch_start, 'seconds')\n",
    "\n",
    "\n",
    "conv_start = time.perf_counter()\n",
    "output = np_Conv2d(input_data=output,\n",
    "                           weights_dict=weights,\n",
    "                           prefix=layers_list[27])\n",
    "conv_end = time.perf_counter()\n",
    "print('Conv layer', conv_end-conv_start, 'seconds')\n",
    "              \n",
    "              \n",
    "batch_start = time.perf_counter()\n",
    "output = np_BatchNorm2d(input_data=output,\n",
    "                            weights_dict=weights,\n",
    "                            prefix=layers_list[28])\n",
    "batch_end = time.perf_counter()\n",
    "output = relu(output)\n",
    "print('Batch layer', batch_end-batch_start, 'seconds')\n",
    "\n",
    "conv_start = time.perf_counter()\n",
    "output = np_Conv2d(input_data=output,\n",
    "                           weights_dict=weights,\n",
    "                           prefix=layers_list[29])\n",
    "conv_end = time.perf_counter()\n",
    "print('Conv layer', conv_end-conv_start, 'seconds')\n",
    "              \n",
    "              \n",
    "              \n",
    "              \n",
    "batch_start = time.perf_counter()\n",
    "output = np_BatchNorm2d(input_data=output,\n",
    "                            weights_dict=weights,\n",
    "                            prefix=layers_list[30])\n",
    "batch_end = time.perf_counter()\n",
    "output = relu(output)\n",
    "print('Batch layer', batch_end-batch_start, 'seconds')\n",
    "\n",
    "\n",
    "conv_start = time.perf_counter()\n",
    "output = np_Conv2d(input_data=output,\n",
    "                           weights_dict=weights,\n",
    "                           prefix=layers_list[31])\n",
    "conv_end = time.perf_counter()\n",
    "print('Conv layer', conv_end-conv_start, 'seconds')\n",
    "              \n",
    "              \n",
    "              \n",
    "batch_start = time.perf_counter()\n",
    "output = np_BatchNorm2d(input_data=output,\n",
    "                            weights_dict=weights,\n",
    "                            prefix=layers_list[32])\n",
    "batch_end = time.perf_counter()\n",
    "output = relu(output)\n",
    "print('Batch layer', batch_end-batch_start, 'seconds')\n",
    "\n",
    "\n",
    "conv_start = time.perf_counter()\n",
    "output = np_Conv2d(input_data=output,\n",
    "                           weights_dict=weights,\n",
    "                           prefix=layers_list[33])\n",
    "conv_end = time.perf_counter()\n",
    "print('Conv layer', conv_end-conv_start, 'seconds')\n",
    "              \n",
    "              \n",
    "              \n",
    "batch_start = time.perf_counter()\n",
    "output = np_BatchNorm2d(input_data=output,\n",
    "                            weights_dict=weights,\n",
    "                            prefix=layers_list[34])\n",
    "batch_end = time.perf_counter()\n",
    "output = relu(output)\n",
    "print('Batch layer', batch_end-batch_start, 'seconds')\n",
    "\n",
    "    \n",
    "\n",
    "conv_start = time.perf_counter()\n",
    "output = np_Conv2d(input_data=output,\n",
    "                           weights_dict=weights,\n",
    "                           prefix=layers_list[35])\n",
    "conv_end = time.perf_counter()\n",
    "print('Conv layer', conv_end-conv_start, 'seconds')\n",
    "\n",
    "              \n",
    "              \n",
    "batch_start = time.perf_counter()\n",
    "output = np_BatchNorm2d(input_data=output,\n",
    "                            weights_dict=weights,\n",
    "                            prefix=layers_list[37])\n",
    "batch_end = time.perf_counter()\n",
    "output = relu(output)\n",
    "print('Batch layer', batch_end-batch_start, 'seconds')\n",
    "    \n",
    "\n",
    "conv_start = time.perf_counter()\n",
    "output = np_Conv2d(input_data=output,\n",
    "                           weights_dict=weights,\n",
    "                           prefix=layers_list[37])\n",
    "conv_end = time.perf_counter()\n",
    "print('Conv layer', conv_end-conv_start, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d889856-fb48-4748-9a71-11ac0b18cc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57b0dfa7-e965-4ff8-9b6e-b47530941f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv Layer 64.96868234901922 seconds\n",
      "Batch Layer 32.9055719919852 seconds\n",
      "Conv Layer 64.09523469401756 seconds\n",
      "Batch Layer 31.545963359996676 seconds\n",
      "Conv Layer 66.69565234996844 seconds\n",
      "Batch Layer 30.97650143801002 seconds\n",
      "Conv Layer 62.27537750697229 seconds\n",
      "Batch Layer 32.84150863398099 seconds\n",
      "Conv Layer 64.75775867199991 seconds\n",
      "Batch Layer 32.42221926501952 seconds\n",
      "Conv Layer 63.86903452302795 seconds\n",
      "Batch Layer 31.691658274969086 seconds\n",
      "Conv Layer 66.01920901599806 seconds\n",
      "Batch Layer 30.85767441795906 seconds\n",
      "Conv Layer 61.23703242902411 seconds\n",
      "Batch Layer 31.696132535988 seconds\n",
      "Conv Layer 61.38363751297584 seconds\n",
      "Batch Layer 32.821852921973914 seconds\n",
      "Conv Layer 61.42835111002205 seconds\n",
      "Batch Layer 32.06110851600533 seconds\n",
      "Conv Layer 67.2828773999936 seconds\n",
      "Batch Layer 32.06524058798095 seconds\n",
      "Conv Layer 64.03477394598303 seconds\n",
      "Batch Layer 31.56302462698659 seconds\n",
      "Conv Layer 62.49593822599854 seconds\n",
      "Batch Layer 32.518785057996865 seconds\n",
      "Conv Layer 64.35918465600116 seconds\n",
      "Batch Layer 31.530662644014228 seconds\n",
      "Conv Layer 63.86104163300479 seconds\n",
      "Batch Layer 31.509301455982495 seconds\n",
      "Conv Layer 63.042073317978065 seconds\n",
      "Batch Layer 31.771844242000952 seconds\n",
      "Conv Layer 63.01200752297882 seconds\n",
      "Batch Layer 31.748101048986427 seconds\n",
      "Conv Layer 64.87141828698805 seconds\n",
      "Batch Layer 31.017571792006493 seconds\n"
     ]
    }
   ],
   "source": [
    "# 1st layer\n",
    "\n",
    "import time \n",
    "\n",
    "output = np_Conv2d(input_data=sample[:1],\n",
    "                   weights_dict=weights,\n",
    "                   prefix=layers_list[0])\n",
    "\n",
    "output = relu(output)\n",
    "\n",
    "\n",
    "for i in range(len(layers_list)-2):\n",
    "    \n",
    "    if layers_list[i+1].endswith('0.'):\n",
    "        \n",
    "        conv_start = time.perf_counter()\n",
    "        output = np_Conv2d(input_data=output,\n",
    "                           weights_dict=weights,\n",
    "                           prefix=layers_list[i+1])\n",
    "        conv_end = time.perf_counter()\n",
    "        print('Conv Layer', conv_end-conv_start, 'seconds')\n",
    "        \n",
    "    elif layers_list[i+1].endswith('1.'):\n",
    "        \n",
    "        batch_start = time.perf_counter()\n",
    "        output = np_BatchNorm2d(input_data=output,\n",
    "                                weights_dict=weights,\n",
    "                                prefix=layers_list[i+1])\n",
    "        output = relu(output)\n",
    "        batch_end = time.perf_counter()\n",
    "        print('Batch Layer', batch_end-batch_start, 'seconds')\n",
    "\n",
    "\n",
    "output = np_Conv2d(input_data=output,\n",
    "                   weights_dict=weights,\n",
    "                   prefix=layers_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bd784d9-cd36-4499-8097-e6a98c2a0053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02869121, -0.03349543, -0.03269177, ..., -0.03265676,\n",
       "        -0.03256562, -0.03074346],\n",
       "       [-0.02880623, -0.02176519, -0.02141996, ..., -0.02140986,\n",
       "        -0.02152639, -0.02576497],\n",
       "       [-0.02915312, -0.02284033, -0.02345272, ..., -0.02346824,\n",
       "        -0.02321174, -0.02718633],\n",
       "       ...,\n",
       "       [-0.02916658, -0.02288682, -0.02355822, ..., -0.02358123,\n",
       "        -0.0233125 , -0.0272828 ],\n",
       "       [-0.0292498 , -0.02327646, -0.02398995, ..., -0.02402493,\n",
       "        -0.02369493, -0.02789586],\n",
       "       [-0.02785992, -0.0164943 , -0.0179336 , ..., -0.01797075,\n",
       "        -0.01794659, -0.02019483]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape\n",
    "output[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2cd07d8-c4e3-46d2-81d3-5fd5d1bb39a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f693cfdd220>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAPMklEQVR4nO3df6hf9X3H8eerNzZ1tlm1NZLdG6aF/DGV1daQpjhGV9eZOWn8R0hhM38IAeegZYOSrGyjjEDXwSiyKQtbMdIfEmjFIHVtyCrbmDReW61GTU1V9JJgaLdi5h9upu/9cT/S0+v95H6T3Nzvt/T5gMM53/c5n3Pe93vNy3M+3y/cVBWStJi3jbsBSZPLgJDUZUBI6jIgJHUZEJK6DAhJXSseEEm2JDmS5GiSnSt9fUmjy0p+DyLJFPAD4GPAHPAo8ImqenrFmpA0spW+g9gEHK2q56vqf4H7gK0r3IOkEa1a4etNAy8PXs8BH1p4UJIdwA6ArH77tResu3RlupN+Cb3xo//m1MnXsti+lQ6IxZp4yzNOVe0B9gCsft90/dpf//H57kv6pXXsL/6hu2+lHzHmgPWD1zPAsRXuQdKIVjogHgU2JLkiyduBbcD+Fe5B0ohW9BGjqt5I8ifAN4Ep4ItVdXgle5A0upWeg6CqvgF8Y6WvK+nM+U1KSV0GhKQuA0JSlwEhqcuAkNRlQEjqMiAkdRkQkroMCEldBoSkLgNCUpcBIanLgJDUZUBI6jIgJHUZEJK6DAhJXQaEpC4DQlKXASGpy4CQ1GVASOoyICR1GRCSugwISV0GhKQuA0JS15IBkeSLSU4keWpQuyTJgSTPtfXFg327khxNciTJDYP6tUmebPvuTJLl/3EkLadR7iDuAbYsqO0EDlbVBuBge02SK4FtwFVtzF1JptqYu4EdwIa2LDynpAmzZEBU1b8B/7WgvBXY27b3AjcP6vdV1etV9QJwFNiUZB2wpqoeqaoC7h2MkTShznYO4rKqOg7Q1mtbfRp4eXDcXKtNt+2FdUkTbLknKRebV6jT1Bc/SbIjyWyS2VOvvrZszUk6M2cbEK+0xwba+kSrzwHrB8fNAMdafWaR+qKqak9VbayqjVNrLjrLFiWdq7MNiP3A9ra9HXhgUN+WZHWSK5ifjDzUHkNOJtncPr24dTBG0oRatdQBSb4KfAR4b5I54K+AzwH7ktwGvATcAlBVh5PsA54G3gDuqKpT7VS3M/+JyIXAQ22RNMGWDIiq+kRn1/Wd43cDuxepzwJXn1F3ksbKb1JK6jIgJHUZEJK6DAhJXQaEpC4DQlKXASGpy4CQ1GVASOoyICR1GRCSugwISV0GhKQuA0JSlwEhqcuAkNRlQEjqMiAkdRkQkroMCEldBoSkLgNCUpcBIanLgJDUZUBI6jIgJHUZEJK6DAhJXUsGRJL1Sb6d5Jkkh5N8stUvSXIgyXNtffFgzK4kR5McSXLDoH5tkifbvjuT5Pz8WJKWwyh3EG8Af1ZVvwFsBu5IciWwEzhYVRuAg+01bd824CpgC3BXkql2rruBHcCGtmxZxp9F0jJbMiCq6nhVfbdtnwSeAaaBrcDedthe4Oa2vRW4r6per6oXgKPApiTrgDVV9UhVFXDvYIykCXRGcxBJLgc+AHwHuKyqjsN8iABr22HTwMuDYXOtNt22F9YXu86OJLNJZk+9+tqZtChpGY0cEEneCXwN+FRVvXq6Qxep1Wnqby1W7amqjVW1cWrNRaO2KGmZjRQQSS5gPhy+XFVfb+VX2mMDbX2i1eeA9YPhM8CxVp9ZpC5pQo3yKUaAfwaeqaq/G+zaD2xv29uBBwb1bUlWJ7mC+cnIQ+0x5GSSze2ctw7GSJpAq0Y45jrgj4Ankzzean8OfA7Yl+Q24CXgFoCqOpxkH/A085+A3FFVp9q424F7gAuBh9oiaUItGRBV9R8sPn8AcH1nzG5g9yL1WeDqM2lQ0vj4TUpJXQaEpC4DQlKXASGpy4CQ1GVASOoyICR1GRCSugwISV0GhKQuA0JSlwEhqcuAkNRlQEjqMiAkdRkQkroMCEldBoSkLgNCUpcBIanLgJDUZUBI6jIgJHUZEJK6DAhJXQaEpC4DQlLXKH/d+x1JDiV5IsnhJJ9t9UuSHEjyXFtfPBizK8nRJEeS3DCoX5vkybbvzvZXviVNqFHuIF4HPlpV7weuAbYk2QzsBA5W1QbgYHtNkiuBbcBVwBbgriRT7Vx3AzuADW3Zsnw/iqTltmRA1Lz/aS8vaEsBW4G9rb4XuLltbwXuq6rXq+oF4CiwKck6YE1VPVJVBdw7GCNpAo00B5FkKsnjwAngQFV9B7isqo4DtPXadvg08PJg+FyrTbfthfXFrrcjyWyS2VOvvnYGP46k5TRSQFTVqaq6Bphh/m7g6tMcvti8Qp2mvtj19lTVxqraOLXmolFalHQenNGnGFX1E+Bh5ucOXmmPDbT1iXbYHLB+MGwGONbqM4vUJU2oUT7FuDTJu9v2hcDvAs8C+4Ht7bDtwANtez+wLcnqJFcwPxl5qD2GnEyyuX16cetgjKQJtGqEY9YBe9snEW8D9lXVg0keAfYluQ14CbgFoKoOJ9kHPA28AdxRVafauW4H7gEuBB5qi6QJtWRAVNX3gQ8sUv8xcH1nzG5g9yL1WeB08xeSJojfpJTUZUBI6jIgJHUZEJK6DAhJXQaEpC4DQlKXASGpy4CQ1GVASOoyICR1GRCSugwISV0GhKQuA0JSlwEhqcuAkNRlQEjqMiAkdRkQkroMCEldBoSkLgNCUpcBIanLgJDUZUBI6jIgJHWNHBBJppJ8L8mD7fUlSQ4kea6tLx4cuyvJ0SRHktwwqF+b5Mm27872V74lTagzuYP4JPDM4PVO4GBVbQAOttckuRLYBlwFbAHuan8ZHOBuYAewoS1bzql7SefVSAGRZAb4A+CfBuWtwN62vRe4eVC/r6per6oXgKPApiTrgDVV9UhVFXDvYIykCTTqHcQXgE8DPx3ULquq4wBtvbbVp4GXB8fNtdp0215Yf4skO5LMJpk99eprI7YoabktGRBJbgJOVNVjI55zsXmFOk39rcWqPVW1sao2Tq25aMTLSlpuq0Y45jrg40luBN4BrEnyJeCVJOuq6nh7fDjRjp8D1g/GzwDHWn1mkbqkCbXkHURV7aqqmaq6nPnJx3+tqj8E9gPb22HbgQfa9n5gW5LVSa5gfjLyUHsMOZlkc/v04tbBGEkTaJQ7iJ7PAfuS3Aa8BNwCUFWHk+wDngbeAO6oqlNtzO3APcCFwENtkTShziggquph4OG2/WPg+s5xu4Hdi9RngavPtElJ4+E3KSV1GRCSugwISV0GhKQuA0JSlwEhqcuAkNRlQEjqMiAkdRkQkroMCEldBoSkLgNCUpcBIanLgJDUZUBI6jIgJHUZEJK6DAhJXQaEpC4DQlKXASGpy4CQ1GVASOoyICR1GRCSugwISV0jBUSSF5M8meTxJLOtdkmSA0mea+uLB8fvSnI0yZEkNwzq17bzHE1yZ/sr35Im1JncQfxOVV1TVRvb653AwaraABxsr0lyJbANuArYAtyVZKqNuRvYAWxoy5Zz/xEknS/n8oixFdjbtvcCNw/q91XV61X1AnAU2JRkHbCmqh6pqgLuHYyRNIFGDYgCvpXksSQ7Wu2yqjoO0NZrW30aeHkwdq7Vptv2wrqkCbVqxOOuq6pjSdYCB5I8e5pjF5tXqNPU33qC+RDaATD1nl8dsUVJy22kO4iqOtbWJ4D7gU3AK+2xgbY+0Q6fA9YPhs8Ax1p9ZpH6YtfbU1Ubq2rj1JqLRv9pJC2rJQMiyUVJ3vXmNvB7wFPAfmB7O2w78EDb3g9sS7I6yRXMT0Yeao8hJ5Nsbp9e3DoYI2kCjfKIcRlwf/tEchXwlar6lySPAvuS3Aa8BNwCUFWHk+wDngbeAO6oqlPtXLcD9wAXAg+1RdKEWjIgqup54P2L1H8MXN8ZsxvYvUh9Frj6zNuUNA5+k1JSlwEhqcuAkNRlQEjqMiAkdRkQkroMCEldBoSkLgNCUpcBIanLgJDUZUBI6jIgJHUZEJK6DAhJXQaEpC4DQlKXASGpy4CQ1GVASOoyICR1GRCSugwISV0GhKQuA0JSlwEhqcuAkNRlQEjqMiAkdRkQkrpWjbuBUWSqxt2C9EspVZP9jy/JSeDIuPsYeC/wo3E3MWA/S5u0niatn1+vqksX2/GLcAdxpKo2jruJNyWZtZ++SesHJq+nSevndJyDkNRlQEjq+kUIiD3jbmAB+zm9SesHJq+nSeuna+InKSWNzy/CHYSkMTEgJHVNbEAk2ZLkSJKjSXaex+t8McmJJE8NapckOZDkuba+eLBvV+vpSJIbBvVrkzzZ9t2ZJGfZz/ok307yTJLDST45zp6SvCPJoSRPtH4+O+73qJ1rKsn3kjw4If282M71eJLZSehpWVTVxC3AFPBD4H3A24EngCvP07V+G/gg8NSg9nlgZ9veCfxN276y9bIauKL1ONX2HQI+DAR4CPj9s+xnHfDBtv0u4AftumPpqY19Z9u+APgOsHmc71E7158CXwEeHPfvrJ3rReC9C2pj7WlZ/n2M8+KnebM/DHxz8HoXsOs8Xu/yBQFxBFjXttcx/2Wtt/QBfLP1ug54dlD/BPCPy9TbA8DHJqEn4FeA7wIfGmc/wAxwEPjoICDG+v50AmLsv7NzXSb1EWMaeHnweq7VVsplVXUcoK3XLtHXdNteWD8nSS4HPsD8/7XH1lO7nX8cOAEcqKqx9gN8Afg08NNBbdy/swK+leSxJDsmpKdzNqlftV7suWsSPo/t9bXs/SZ5J/A14FNV9eppHkXPe09VdQq4Jsm7gfuTXH2aw89rP0luAk5U1WNJPjLKkPPZz8B1VXUsyVrgQJJnJ6CnczapdxBzwPrB6xng2Ape/5Uk6wDa+sQSfc217YX1s5LkAubD4ctV9fVJ6Amgqn4CPAxsGWM/1wEfT/IicB/w0SRfGmM/AFTVsbY+AdwPbBp3T8tinM83p3meWwU8z/wEzpuTlFedx+tdzs/PQfwtPz+59Pm2fRU/P7n0PD+bXHqU+cm7NyeXbjzLXgLcC3xhQX0sPQGXAu9u2xcC/w7cNM73aNDbR/jZHMQ4f2cXAe8abP8n8yE69vfonP9tjPPiS7zpNzI/g/9D4DPn8TpfBY4D/8d8gt8GvIf5SbDn2vqSwfGfaT0dYTDDDGwEnmr7/p72LdWz6Oe3mL+t/D7weFtuHFdPwG8C32v9PAX8ZauP7T0anG8YEOP8nb2v/YN/Ajj85n+vk/AeneviV60ldU3qHISkCWBASOoyICR1GRCSugwISV0GhKQuA0JS1/8DUay5tKVoL8YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(output[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f60d6a52-1b0a-4fa5-a9e6-4f5ae57c45cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.1.0.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_216701/2499451246.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'output = model_flow(input_data=sample[:1], weights_dict=weights, prefix_list=layers_list)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/global/common/software/nersc/shasta2105/pytorch/1.9.0/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2346\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2347\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2348\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2349\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/software/nersc/shasta2105/pytorch/1.9.0/lib/python3.8/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/software/nersc/shasta2105/pytorch/1.9.0/lib/python3.8/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/software/nersc/shasta2105/pytorch/1.9.0/lib/python3.8/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0mnumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                 \u001b[0mtime_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/software/nersc/shasta2105/pytorch/1.9.0/lib/python3.8/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_216701/2499451246.py\u001b[0m in \u001b[0;36mmodel_flow\u001b[0;34m(input_data, weights_dict, prefix_list)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprefix_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'0.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             output = np_Conv2d(input_data=output,\n\u001b[0m\u001b[1;32m     14\u001b[0m                                \u001b[0mweights_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                prefix=prefix_list[i+1])\n",
      "\u001b[0;32m/tmp/ipykernel_216701/2137819419.py\u001b[0m in \u001b[0;36mnp_Conv2d\u001b[0;34m(input_data, weights_dict, prefix, stride, padding, dilation)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrelate2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/common/software/nersc/shasta2105/pytorch/1.9.0/lib/python3.8/site-packages/scipy/signal/signaltools.py\u001b[0m in \u001b[0;36mcorrelate2d\u001b[0;34m(in1, in2, mode, boundary, fillvalue)\u001b[0m\n\u001b[1;32m   1784\u001b[0m     \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_valfrommode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m     \u001b[0mbval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_bvalfromboundary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboundary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1786\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convolve2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfillvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mswapped_inputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def model_flow(input_data, weights_dict, prefix_list):\n",
    "    \n",
    "    output = np_Conv2d(input_data=input_data,\n",
    "                   weights_dict=weights_dict,\n",
    "                   prefix=prefix_list[0])\n",
    "\n",
    "\n",
    "    for i in range(len(prefix_list)-2):\n",
    "        print(prefix_list[i+1])\n",
    "\n",
    "        if prefix_list[i+1].endswith('0.'):\n",
    "\n",
    "            output = np_Conv2d(input_data=output,\n",
    "                               weights_dict=weights_dict,\n",
    "                               prefix=prefix_list[i+1])\n",
    "\n",
    "        elif layers_list[i+1].endswith('1.'):\n",
    "            output = np_BatchNorm2d(input_data=output,\n",
    "                                    weights_dict=weights_dict,\n",
    "                                    prefix=prefix_list[i+1])\n",
    "\n",
    "    output = np_Conv2d(input_data=output,\n",
    "                       weights_dict=weights_dict,\n",
    "                       prefix=prefix_list[-1])\n",
    "    \n",
    "    return output\n",
    "\n",
    "%timeit output = model_flow(input_data=sample[:1], weights_dict=weights, prefix_list=layers_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1709dad8-ea09-495e-8718-1baf7c8e331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st layer\n",
    "output = np_Conv2d(input_data=sample[:1],\n",
    "                   weights_dict=weights,\n",
    "                   prefix=layers_list[0])\n",
    "\n",
    "[np_Conv2d(input_data=output,\n",
    "           weights_dict=weights,\n",
    "           prefix=layers_list[i+1])\n",
    " \n",
    "if layers_list[i+1].endswith('0.') else\n",
    " \n",
    " np_BatchNorm2d(input_data=output,\n",
    "                weights_dict=weights,\n",
    "                prefix=layers_list[i+1])\n",
    " \n",
    "for i in range(len(layers_list)-2)]\n",
    "\n",
    "output = np_Conv2d(input_data=output, \n",
    "                   weights_dict=weights,\n",
    "                   prefix=layers_lust[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285fff0c-e438-4bf4-8a12-d2703ccfef0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e03064-004c-4614-9399-21e915623d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-1.9.0",
   "language": "python",
   "name": "pytorch-1.9.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
